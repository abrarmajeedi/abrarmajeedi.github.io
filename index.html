<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Abrar Majeedi</title>

    <meta name="author" content="Abrar Majeedi">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Abrar Majeedi
                </p>
                <p>I'm a PhD candidate at the University of Wisconsin-Madison, working on multimodal deep learning.  I am fortunate to be advised by Prof. Yin Li.</p>
                </p>
                <p>
                 I have also gained valuable research experience through internships at Amazon, Microsoft and Dell EMC.
                </p>
                <p style="text-align:center">
                  <a href="mailto:majeedi@wisc.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Abrar_Resume.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.co.in/citations?user=pcGoUxoAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/abrarmajeedi/">Github</a>
                </p>
              </td>
<td style="padding:2.5%;width:40%;max-width:40%">
  <a href="images/abrar_img.JPEG">
    <img 
      src="images/abrar_headshot.jpeg" 
      alt="profile photo"
      style="width:95%; aspect-ratio: 1 / 1; object-fit: cover; object-position: center 30%; border-radius: 50%; transform: scale(1.0);" 
      class="hoverZoomLink"
    >
  </a>
</td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research focus is multimodal deep learning for sequential data. I am also interested in Vision Language Models, Large Language Models, and their applications in sequential data such as videos, sensor data (time series) and text.
                  <!-- Some papers are <span class="highlight">highlighted</span>. -->
                </p>
              </td>
            </tr>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>News and Updates</h2>
                <ul>
                  <li style="margin-bottom: 10px;">July 2025: Our paper <i>"Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering"</i> accepted as an Oral presentation at BioNLP @ ACL 2025.</li>
                  <li style="margin-bottom: 10px;">June 2025: Our team was awarded 2nd position in the Shared Task on grounded question answering (QA) from electronic health records (<a href="https://archehr-qa.github.io/">ArchEHR-QA 2025</a>) at BioNLP@ACL 2025.</li>
                  <li style="margin-bottom: 10px;">June 2025: Our paper <i>"LETS Forecast: Learning Embedology for Time Series Forecasting"</i> has been accepted at the <b>International Conference on Machine Learning (ICML)</b> 2025.</li>
                  <li style="margin-bottom: 10px;">April 2025: Our team was awarded <b>1st place</b> in the Machine Learning Challenge at the Pediatric Academic Societies (PAS) 2025 Conference, Honolulu, Hawaii.</li>
                  <li style="margin-bottom: 10px;">Jan 2025: Our Poster <i>"Deep learning to quantify care manipulation activities in neonatal intensive care units"</i> won an <b>Award for Best Innovation in Neonatology</b> at the Cleveland Clinic Children's SHINE (Syposium on Health Innovation and Neonatal Excellence) Conference, Orlando, FL.</li>

                  <li style="margin-bottom: 10px;">Nov 2024: Our paper <i>"RICA2: Rubric-Informed, Calibrated Assessment of Actions"</i> won the <b>Best Poster Award</b> at NSF Poster Competition at Purdue University, West Lafayette, IN.</li>
                </ul>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            
    <!--<tr bgcolor="#ffffd0"></tr> -->
    <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id="epic_2022_video">
              <img id="takens_theorem" src="./deep_edm/img/taken.gif" alt="Takens Theorem GIF" style="width:100%">
            </div>
          </div>
        </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://abrarmajeedi.github.io/deep_edm/">
          <span class="papertitle">LETS Forecast: Learning Embedology for Time Series Forecasting</span>
        </a>
        <br>
				<strong>Abrar Majeedi</strong>,
        <a href="">Viswanatha Reddy Gajjala</a>, 
        <a href="">Satya Sai Srinath Namburi GNVV</a>,
        <a href="">Nada Elkordi</a>,
        <a href="https://www.biostat.wisc.edu/~yli/">Yin Li</a>
        <br>
        <em>International Conference on Machine Learning (ICML)</em> 2025
        <br>
        <a href="https://abrarmajeedi.github.io/">Project Page</a> /
        <a href="https://arxiv.org/abs/2506.06454">Paper Link</a>
        <p></p>
        <p>A novel time series forecasting method that combines principles from nonlinear dynamical systems with deep learning to model latent temporal structure for accurate forecasts.</p>
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='rica2_icon'>
            <img src="images/rica2_icon.png" width="100%" />
          </div>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://abrarmajeedi.github.io/rica2_aqa/">
			<span class="papertitle">RICA<sup>2</sup>: Rubric-Informed, Calibrated Assessment of Actions
</span>
        </a>
        <br>
				<strong>Abrar Majeedi</strong>,
        <a href="">Viswanatha Reddy Gajjala</a>, 
        <a href="">Satya Sai Srinath Namburi GNVV</a>,
        <a href="https://www.biostat.wisc.edu/~yli/">Yin Li</a>, 
        <br>
        <em>European Conference on Computer Vision (ECCV)</em> 2024
        <br>
        <a href="https://abrarmajeedi.github.io/rica2_aqa/">Project page</a>
        /
        <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08001.pdf">Paper Link</a>
        <p></p>
        <p>
				Action quality assessment in videos by incorporating human designed scoring rubrics while providing calibrated uncertainty estimates.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='npj_icon'>
            <img src="images/j_of_p_figure.jpg" width="100%" />
          </div>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
			<span class="papertitle">Glottic Opening Detection using Deep Learning for Neonatal Intubation with Video Laryngoscopy
</span>
        </a>
        <br>
				<strong>Abrar Majeedi</strong>,
        <a href="https://www.pediatrics.wisc.edu/staff/peebles-md-patrick/">Patrick Peebles</a>,
        <a href="https://www.biostat.wisc.edu/~yli/">Yin Li</a>,
        <a href="https://www.pediatrics.wisc.edu/staff/mcadams-ryan/">Ryan McAdams</a>
        <br>
        <em>Nature - Journal of Perinatology</em>, Nov 2024
        <br>
        <a href="https://www.nature.com/articles/s41372-024-02171-3">Paper link</a>
        <p></p>
        <p>
				Auotmatically detect the glottic opening during neonatal intubation using video laryngoscopy to improve intubation outcomes.
        </p>
      </td>
    </tr>



    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='npj_icon'>
            <img src="images/npj_icon.png" width="100%" />
          </div>
        </div>
      </td>
     
      <td style="padding:20px;width:75%;vertical-align:middle">

        <a href="https://www.nature.com/articles/s41746-024-01164-y">
          <span class="papertitle">Deep learning to quantify care manipulation activities in neonatal intensive care units
</span>
        </a>
        <br>
				<strong>Abrar Majeedi</strong>,
				<a href="https://www.pediatrics.wisc.edu/staff/mcadams-ryan/">Ryan McAdams</a>,
				Ravneet Kaur,
				Shubham Gupta,
				Harpreet Singh,
				<a href="https://www.biostat.wisc.edu/~yli/">Yin Li</a>
        <br>
        <em>npj Digital Medicine</em>, June 2024
        <br>
        <a href=" https://www.nature.com/articles/s41746-024-01164-y.pdf">Paper Link</a>
        /
        <a href="https://github.com/abrarmajeedi/DL-to-quantify-manipulation-activities-in-NICUs">Code</a>
        <p></p>
        <p>
        Automatically quantify care manipulation activities in neonatal intensive care units (NICUs), while integrating physiological 
        signal data to monitor neonatal stress in NICUs.
        </p>
      </td>
    </tr>


      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='npj_icon'>
              <img src="images/mlcvqa_icon.png" width="100%" />
            </div>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2309.00769">
        <span class="papertitle">Full Reference Video Quality Assessment for Machine Learning-Based Video Codecs
  </span>
          </a>
          <br>
          <strong>Abrar Majeedi</strong>,
          <a href="https://www.microsoft.com/en-us/research/people/babaknaderi/">Babak Naderi</a>, 
          <a href="">Yasaman Hosseinkashi</a>,
          <a href="">Juhee Cho</a>, 
          <a href="">Ruben Alvarez Martinez</a>,
          <a href="https://www.microsoft.com/en-us/research/people/rcutler/">Ross Cutler</a>
          <br>
          <em>Preprint</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2309.00769">Paper</a>
          /
          <a href="https://github.com/microsoft/MLCVQA">Code</a>
          <p></p>
          <p>
          Assess perceptual quality of videos encoded by Machine Learning-based Video Codecs.
          </p>
        </td>
      </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='epic_2022_video'>
            <video id="epic_video" width="100%" muted>
              <source src="images/epic_2022_video.webm" type="video/webm">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://epic-kitchens.github.io/2022#results">
          <span class="papertitle">Detecting Egocentric Actions with ActionFormer</span>
        </a>
        <br>
        <a href="">Chenlin Zhang</a>, 
        <a href="">Lin Sui</a>,
        <strong>Abrar Majeedi</strong>,
        <a href="">Viswantha Reddy Gajjala</a>,
        <a href="https://www.biostat.wisc.edu/~yli/">Yin Li</a><br>
        <em>EPIC @ CVPR Workshop</em> 2022
        <br>
        <a href="https://epic-kitchens.github.io/Reports/EPIC-KITCHENS-Challenges-2022-Report.pdf">Report</a>
        /
        <a href="https://github.com/happyharrycn/actionformer_release">Code</a>
        <p></p>
        <p>Won 2nd place in the Egocentric action detection challenge</p>
      </td>
    </tr>
  </table>

  

  <script>
    const videoElement = document.getElementById('epic_video');

    videoElement.addEventListener('mouseover', function() {
      videoElement.play();
    });

    videoElement.addEventListener('mouseout', function() {
      videoElement.pause();
      videoElement.currentTime = 0; // Optional: reset to the beginning of the video
    });
  </script>
           
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:cneter;font-size:small;">
                  <a href="https://info.flagcounter.com/145E"><img src="https://s05.flagcounter.com/count2/145E/bg_FFFFFF/txt_000000/border_CCCCCC/columns_3/maxflags_27/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a> <br>
                  <img src="https://badges.toozhao.com/badges/01JAV44T8MK606X86QFKFEE3K6/green.svg" /> <br>
                  Website template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
